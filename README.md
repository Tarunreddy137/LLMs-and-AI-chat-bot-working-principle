# README: Natural Language Processing Concepts and Tools Overview

## Tokenization

Tokenization is the process of breaking down text into smaller units, such as words or subwords, for language processing tasks.

1. **Word Piece**
2. **Byte Pair Encoding (BPE)**
3. **Unigram Language Model (UnigramLM)**

## Positional Encoding

Transformers process input sequences independently and in parallel, utilizing techniques to encode positional information efficiently.

1. **Alibi**: Applies a scalar bias to attention scores based on token position distance.
2. **Rope**: Rotates query and key representations proportionally to token absolute position.

## Attention Mechanisms in LLMs

Attention mechanisms assign weights to input tokens based on their relevance to the model.

1. **Self Attention**
2. **Cross Attention**
3. **Sparse Attention**
4. **Flash Attention**

## Activation Functions

Activation functions introduce non-linearity into neural networks.

1. **ReLU (Rectified Linear Unit)**
2. **GELU (Gaussian Error Linear Unit)**
3. **GLU Variants (Gated Linear Unit)**

## Distributed LLM Training

Training large language models efficiently across multiple devices or processors.

1. **Data Parallelism**
2. **Tensor Parallelism**
3. **Pipeline Parallelism**
4. **Model Parallelism**
5. **3D Parallelism**
6. **Optimizer Parallelism**

## Data Preprocessing

Preparing data for training and inference in NLP tasks.

1. **Classifier-based**: Using classifiers to filter text based on quality.
2. **Heuristics-based**: Applying rules and metrics to filter text.

## Data Deduplication

Removing duplicate data instances from datasets.

## Architectures

Different architectural designs for processing and generating text.

1. **Encoder-Decoder**
2. **Causal Decoder**
3. **Prefix Decoder**

## Pre-Training Objectives

Objectives used during pre-training of language models.

1. **Full Language Modelling**
2. **Prefix Language Modelling**
3. **Masked Language Modelling**
4. **Unified Language Modelling**

## Fine-Tuning

Adapting pre-trained models to specific tasks or domains.

1. **Transfer Learning**
2. **Instruction Tuning**
3. **Alignment Tuning**

## Pretrained LLMs

Overview of popular pretrained large language models.

1. **T5**
2. **GPT-3**
3. **mT5**
4. **PanGu-Alpha**
5. **CPM-2**
6. **Codex**
7. **ERNIE 3.0**

## AI Chatbot Working Principles

Fundamental principles behind the operation of AI chatbots.

---

This README provides an overview of various concepts, tools, and models used in Natural Language Processing (NLP), highlighting their applications and methodologies.

